import numpy as np
import re

# -------- a. Data Preparation --------
sentences = """We are about to study the idea of a computational process.
Computational processes are abstract beings that inhabit computers.
As they evolve, processes manipulate other abstract things called data.
The evolution of a process is directed by a pattern of rules
called a program. People create programs to direct processes. In effect,
we conjure the spirits of the computer with our spells."""

# Clean and preprocess text
sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)
sentences = re.sub(r'(?:^| )\w(?:$| )', ' ', sentences).strip()
sentences = sentences.lower()
words = sentences.split()

vocab = set(words)
vocab_size = len(vocab)
embed_dim = 10
context_size = 2

word_to_ix = {word: i for i, word in enumerate(vocab)}
ix_to_word = {i: word for i, word in enumerate(vocab)}

# -------- b. Generate Training Data --------
data = []
for i in range(2, len(words) - 2):
    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]
    target = words[i]
    data.append((context, target))

print(data[:5])

# -------- c. Train Model --------
embeddings = np.random.random_sample((vocab_size, embed_dim))
theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))

def log_softmax(x):
    e_x = np.exp(x - np.max(x))
    return np.log(e_x / e_x.sum())

def forward(context_idxs, theta):
    m = embeddings[context_idxs].reshape(1, -1)
    n = np.dot(m, theta)
    e_x = np.exp(n - np.max(n))
    o = np.log(e_x / e_x.sum())
    return m, n, o

# -------- d. Output --------
for context, target in data[:3]:
    context_idxs = [word_to_ix[w] for w in context]
    m, n, o = forward(context_idxs, theta)
    print(f"\nContext: {context}")
    print(f"Predicted log-probabilities:\n{o}")
